{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Task 1 - Gradient Descent\n",
    " \n",
    "\n",
    "**Warm-up exercise**: Implement the gradient descent update rule. The  gradient descent rule is, for $l = 1, ..., L$: \n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{1}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{2}$$\n",
    "\n",
    "where L is the number of layers and $\\alpha$ is the learning rate. All parameters should be stored in the `parameters` dictionary. Note that the iterator `l` starts at 0 in the `for` loop while the first parameters are $W^{[1]}$ and $b^{[1]}$. You need to shift `l` to `l+1` when coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters_with_gd\n",
    "\n",
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using one step of gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters to be updated:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients to update each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 =\n",
      "[[ 1.39244598 -0.61290113 -0.52907334]\n",
      " [-1.07347111  0.86450677 -2.78085497]]\n",
      "b1 =\n",
      "[[ 1.74493465]\n",
      " [-0.76027113]]\n"
     ]
    }
   ],
   "source": [
    "#assign the value of parameters and grads.Do it in this cell itself so that the resulting parameters don't change every time you run this cell\n",
    "parameters={'W1': array([[ 1.39134536, -0.61175641, -0.52817175],\n",
    "         [-1.07296862,  0.86540763, -2.7815387 ]]),\n",
    "  'W2': array([[ 0.3190391 , -0.24937038,  1.46210794],\n",
    "         [-2.06014071, -0.3224172 , -0.38405435],\n",
    "         [ 1.13376944, -1.09989127, -0.17242821]]),\n",
    "  'b1': array([[ 1.74481176],\n",
    "         [-0.7612069 ]]),\n",
    "  'b2': array([[-0.87785842],\n",
    "         [ 0.04221375],\n",
    "         [ 0.58281521]])}\n",
    "grads={'dW1': array([[-1.10061918,  1.14472371,  0.90159072],\n",
    "         [ 0.50249434,  0.90085595, -0.68372786]]),\n",
    "  'dW2': array([[-0.21788808,  0.53035547, -0.69166075],\n",
    "         [-0.39675353, -0.6871727 , -0.84520564],\n",
    "         [-0.67124613, -0.0126646 , -1.11731035]]),\n",
    "  'db1': array([[-0.12289023],\n",
    "         [-0.93576943]]),\n",
    "  'db2': array([[ 0.2844157 ],\n",
    "         [ 1.78980218],\n",
    "         [ 0.79204416]])}\n",
    "learning_rate=0.001\n",
    "\n",
    "parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "print(\"W1 =\\n\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 =\\n\" + str(parameters[\"b1\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Q) .Run the function :update_parameters_with_momentum() with parameters and grads same as in the link in previous question,v initialized to zero ,beta=0.9 ,learning_rate=0.01.What is the value of the second element of the array v[\"db2\"] ?(rounded to 3 decimal places).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Momentum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Exercise**: Initialize the velocity. The velocity, $v$, is a python dictionary that needs to be initialized with arrays of zeros. Its keys are the same as those in the `grads` dictionary, that is:\n",
    "for $l =1,...,L$:\n",
    "```python\n",
    "v[\"dW\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l+1)])\n",
    "v[\"db\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l+1)])\n",
    "```\n",
    "**Note** that the iterator l starts at 0 in the for loop while the first parameters are v[\"dW1\"] and v[\"db1\"] (that's a \"one\" on the superscript). This is why we are shifting l to l+1 in the `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    \"\"\"\n",
    "    Initializes the velocity as a python dictionary with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    \n",
    "    Returns:\n",
    "    v -- python dictionary containing the current velocity.\n",
    "                    v['dW' + str(l)] = velocity of dWl\n",
    "                    v['db' + str(l)] = velocity of dbl\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    \n",
    "    # Initialize velocity\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v[\"dW1\"] =\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "v[\"db1\"] =\n",
      "[[0.]\n",
      " [0.]]\n",
      "v[\"dW2\"] =\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "v[\"db2\"] =\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "v = initialize_velocity(parameters)\n",
    "print(\"v[\\\"dW1\\\"] =\\n\" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] =\\n\" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] =\\n\" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] =\\n\" + str(v[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:  Now, implement the parameters update with momentum. The momentum update rule is, for $l = 1, ..., L$: \n",
    "\n",
    "$$ \\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n",
    "\\end{cases}\\tag{3}$$\n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\n",
    "b^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}} \n",
    "\\end{cases}\\tag{4}$$\n",
    "\n",
    "where L is the number of layers, $\\beta$ is the momentum and $\\alpha$ is the learning rate. All parameters should be stored in the `parameters` dictionary.  Note that the iterator `l` starts at 0 in the `for` loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that's a \"one\" on the superscript). So you will need to shift `l` to `l+1` when coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using Momentum\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- python dictionary containing the current velocity:\n",
    "                    v['dW' + str(l)] = ...\n",
    "                    v['db' + str(l)] = ...\n",
    "    beta -- the momentum hyperparameter, scalar\n",
    "    learning_rate -- the learning rate, scalar\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- python dictionary containing your updated velocities\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "        \n",
    "        # compute velocities\n",
    "        v[\"dW\" + str(l+1)] = beta * v[\"dW\" + str(l+1)] + (1 - beta) * grads['dW' + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta * v[\"db\" + str(l+1)] + (1 - beta) * grads['db' + str(l+1)]\n",
    "        # update parameters\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v[\"dW\" + str(l+1)] \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v[\"db\" + str(l+1)] \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v[\"dW1\"] =\n",
      "[[-0.11006192  0.11447237  0.09015907]\n",
      " [ 0.05024943  0.09008559 -0.06837279]]\n",
      "v[\"db1\"] =\n",
      "[[-0.01228902]\n",
      " [-0.09357694]]\n",
      "v[\"dW2\"] =\n",
      "[[-0.02178881  0.05303555 -0.06916607]\n",
      " [-0.03967535 -0.06871727 -0.08452056]\n",
      " [-0.06712461 -0.00126646 -0.11173103]]\n",
      "v[\"db2\"] =\n",
      "[[0.02844157]\n",
      " [0.17898022]\n",
      " [0.07920442]]\n"
     ]
    }
   ],
   "source": [
    "parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0)\n",
    "print(\"v[\\\"dW1\\\"] =\\n\" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] =\\n\" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] =\\n\" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] =\\n\" + str(v[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
